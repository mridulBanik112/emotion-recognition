{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DIR = 'J:/SONGS/LAP DOWNLOAD/GEETA/Compressed/Data of saudia/Data of saudia'\n",
    "TRAIN_DIR = 'C:/Users/DELL/SkyDrive/Documents/data2/train_data'\n",
    "# TEST_DIR = 'J:/SONGS/LAP DOWNLOAD/GEETA/Compressed/Data of saudia/tes'\n",
    "TEST_DIR = 'C:/Users/DELL/SkyDrive/Documents/data2/test_data'\n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3\n",
    " \n",
    " \n",
    "'''Setting up the model which will help with tensorflow models'''\n",
    "MODEL_NAME = 'Autistic-{}-{}.model'.format(LR, '6conv-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    word_label = img.split('.')#[-3]\n",
    "    s=word_label[0]\n",
    " # DIY One hot encoder\n",
    "    if 'happy' in s : \n",
    "        return [1, 0, 0, 0]\n",
    "    elif 'sad' in  s: \n",
    "        return [0, 1, 0, 0]\n",
    "    elif 'angry' in  s: \n",
    "        return [0, 0, 1, 0] \n",
    "    elif 'normal' in  s: \n",
    "        return [0, 0, 1, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    # Creating an empty list where we should the store the training data\n",
    "    # after a little preprocessing of the data\n",
    "    training_data = []\n",
    " \n",
    "    # tqdm is only used for interactive loading\n",
    "    # loading the training data\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    " \n",
    "        # labeling the images\n",
    "        label = label_img(img)\n",
    " \n",
    "        path = os.path.join(TRAIN_DIR, img)\n",
    " \n",
    "        # loading the image from the path and then converting them into\n",
    "        # greyscale for easier covnet prob\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    " \n",
    "        # resizing the image for processing them in the covnet\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    " \n",
    "        # final step-forming the training data list with numpy array of the images\n",
    "        training_data.append([np.array(img), np.array(label)])\n",
    " \n",
    "    # shuffling of the training data to preserve the random state of our data\n",
    "    shuffle(training_data)\n",
    " \n",
    "    # saving our trained data for further uses if required\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data():\n",
    "    testing_data = []\n",
    "    for img in tqdm(os.listdir(TEST_DIR)):\n",
    "        path = os.path.join(TEST_DIR, img)\n",
    "        img_num = img.split('.')[0]\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "         \n",
    "    shuffle(testing_data)\n",
    "    np.save('test_data.npy', testing_data)\n",
    "    return testing_data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 413/413 [00:03<00:00, 115.80it/s]\n",
      "100%|███████████████████████████████████████| 105/105 [00:00<00:00, 105.24it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Running the training and the testing in the dataset for our model'''\n",
    "train_data = create_train_data()\n",
    "test_data = process_test_data()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]\n",
    "type(train_data)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    " \n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "convnet = input_data(shape =[None, IMG_SIZE, IMG_SIZE, 1], name ='input')\n",
    " \n",
    "convnet = conv_2d(convnet, 32, 5, activation ='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    " \n",
    "convnet = conv_2d(convnet, 64, 5, activation ='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    " \n",
    "convnet = conv_2d(convnet, 128, 5, activation ='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    " \n",
    "convnet = conv_2d(convnet, 64, 5, activation ='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    " \n",
    "convnet = conv_2d(convnet, 32, 5, activation ='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    " \n",
    "convnet = fully_connected(convnet, 1024, activation ='relu')\n",
    "convnet = dropout(convnet, 0.3)\n",
    " \n",
    "convnet = fully_connected(convnet, 4, activation ='softmax')\n",
    "convnet = regression(convnet, optimizer ='adam', learning_rate = LR,\n",
    "      loss ='categorical_crossentropy', name ='targets')\n",
    " \n",
    "model = tflearn.DNN(convnet, tensorboard_dir ='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomX shape (413, 2500)\n",
      "(413, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random classifier model implementation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "# clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "RandomX = np.array([i[0] for i in train_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "RandomY = [i[1] for i in train_data]\n",
    "# RandomX=RandomX.reshape(27,2500)\n",
    "RandomX=RandomX.reshape(len(train_data),IMG_SIZE*IMG_SIZE)\n",
    "print(\"RandomX shape\",RandomX.shape)\n",
    "\n",
    "#  svm y editing\n",
    "svm_y = []\n",
    "for val in RandomY:\n",
    "    if np.argmax(val) == 0:\n",
    "#         str_label ='Happy'\n",
    "        svm_y.append(0)\n",
    "    elif np.argmax(val) == 1:\n",
    "#         str_label ='sad'\n",
    "        svm_y.append(1)\n",
    "    elif np.argmax(val) == 2:\n",
    "#         str_label ='Angry'\n",
    "        svm_y.append(2)\n",
    "    elif np.argmax(val) == 3:\n",
    "#         str_label ='normal'\n",
    "        svm_y.append(3)\n",
    "\n",
    "\n",
    "RandomY_numpy_array = np.array(RandomY)\n",
    "# type(RandomX)\n",
    "# print((RandomY))\n",
    "# print((RandomY.shape))\n",
    "print((RandomY_numpy_array.shape))\n",
    "# print((RandomY.ndim))\n",
    "clf.fit(RandomX,RandomY)\n",
    "s_v_m = svm.SVC(gamma=0.001,C=100)\n",
    "s_v_m.fit(RandomX,svm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)\n",
    "int(len(train_data) * .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the testing data and training data\n",
    "split_value=int(len(train_data) * .3) # 30 /70 split \n",
    "train = train_data[:-split_value]\n",
    "test = train_data[-split_value:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-Features & Y-Labels\n",
    " \n",
    "X = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Y = [i[1] for i in train]\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "test_y = [i[1] for i in test]\n",
    "# print(\"test_y\",len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1074  | total loss: 0.58143 | time: 0.811s\n",
      "| Adam | epoch: 215 | loss: 0.58143 - acc: 0.9447 -- iter: 256/290\n",
      "Training Step: 1075  | total loss: 0.56581 | time: 2.091s\n",
      "| Adam | epoch: 215 | loss: 0.56581 - acc: 0.9424 | val_loss: 2.58254 - val_acc: 0.6585 -- iter: 290/290\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\DELL\\Autistic-0.001-6conv-basic.model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model_details = model.fit({'input': X}, {'targets': Y}, n_epoch = 10, \n",
    "    validation_set =({'input': test_x}, {'targets': test_y}), \n",
    "    snapshot_step = 2000, show_metric = True, run_id = MODEL_NAME)\n",
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 3, 2, 3, 0, 0, 2, 3, 1, 2, 1, 3, 1, 1, 3, 3, 3, 1, 1, 1, 2, 1, 3, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 0, 0, 1, 2, 3, 3, 1, 1, 2, 1, 0, 3, 2, 1, 0, 3, 2, 0, 2, 1, 1, 2, 0, 0, 1, 1, 1, 2, 0, 2, 0, 3, 0, 2, 0, 3, 2, 1, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 3, 0, 0, 2, 1, 3, 2, 0, 0, 2, 3, 3, 3, 3]\n",
      "[2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 0, 1, 2, 0, 2, 1, 1, 2, 2, 1, 2, 2, 2, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 0]\n",
      "Accuracy cnn : 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "'''Testing the data'''\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "# if you need to create the data:\n",
    "# test_data = process_test_data()\n",
    "# if you already have some saved:\n",
    "test_data = np.load('test_data.npy')\n",
    " \n",
    "# fig = plt.figure()\n",
    "Label_OF_test_data = [] \n",
    "Predicted_label = []\n",
    "Predicted_label_RandomForest = []\n",
    "b = True\n",
    "\n",
    "\n",
    "for num, data in enumerate(test_data):\n",
    "    # cat: [1, 0]\n",
    "    # dog: [0, 1]\n",
    "     \n",
    "    img_num = data[1]\n",
    "    img_data = data[0]\n",
    "#     print(img_num)\n",
    "    if 'happy' in img_num : \n",
    "        Label_OF_test_data.append(0)\n",
    "    elif 'sad' in  img_num: \n",
    "        Label_OF_test_data.append(1)\n",
    "    elif 'angry' in  img_num: \n",
    "        Label_OF_test_data.append(2)\n",
    "    elif 'normal' in  img_num: \n",
    "        Label_OF_test_data.append(3)\n",
    "        \n",
    "    \n",
    "#     y = fig.add_subplot(4, 5, num + 1)\n",
    "    orig = img_data\n",
    "    data = img_data.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    " \n",
    "    # model_out = model.predict([data])[0]\n",
    "#     print(data.shape)\n",
    "#     print(data.ndim)\n",
    "    \n",
    "    dataRC=data.reshape(1,IMG_SIZE*IMG_SIZE)\n",
    "#     print(dataRC.shape)\n",
    "#     print(dataRC.ndim)\n",
    "#     if b:\n",
    "#         break\n",
    "    model_out = model.predict([data])[0]\n",
    "#     print(np.argmax(model_out))\n",
    "    if np.argmax(model_out) == 0:\n",
    "#         str_label ='Happy'\n",
    "        Predicted_label.append(0)\n",
    "    elif np.argmax(model_out) == 1:\n",
    "#         str_label ='sad'\n",
    "        Predicted_label.append(1)\n",
    "    elif np.argmax(model_out) == 2:\n",
    "#         str_label ='Angry'\n",
    "        Predicted_label.append(2)\n",
    "    elif np.argmax(model_out) == 3:\n",
    "#         str_label ='normal'\n",
    "        Predicted_label.append(3)\n",
    "    \n",
    "        \n",
    "\n",
    "score=metrics.accuracy_score(Label_OF_test_data, Predicted_label)\n",
    "print(Label_OF_test_data)\n",
    "print(Predicted_label)\n",
    "print('Accuracy cnn : {}'.format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(105, 50, 50, 1)\n",
      "2\n",
      "(105, 2500)\n",
      "[2, 3, 1, 3, 2, 3, 0, 0, 2, 3, 1, 2, 1, 3, 1, 1, 3, 3, 3, 1, 1, 1, 2, 1, 3, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 0, 0, 1, 2, 3, 3, 1, 1, 2, 1, 0, 3, 2, 1, 0, 3, 2, 0, 2, 1, 1, 2, 0, 0, 1, 1, 1, 2, 0, 2, 0, 3, 0, 2, 0, 3, 2, 1, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 3, 0, 0, 2, 1, 3, 2, 0, 0, 2, 3, 3, 3, 3]\n",
      "[2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2]\n",
      "Accuracy RANDOM CLASSIFIER: 0.4\n"
     ]
    }
   ],
   "source": [
    "# RANDOM CLASSIFIER PREDICTION\n",
    "\n",
    "test_Xrc = np.array([i[0] for i in test_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "print(test_Xrc.ndim)\n",
    "print(test_Xrc.shape)\n",
    "test_Xrc=test_Xrc.reshape(len(test_Xrc),IMG_SIZE*IMG_SIZE)\n",
    "print(test_Xrc.ndim)\n",
    "print(test_Xrc.shape)\n",
    "# *************************IMPLEMENTING rANDOM fOREST MODEL ******************************************\n",
    "pred_rc=clf.predict(test_Xrc)\n",
    "# print(pred_rc)\n",
    "final_pred_rc=[]\n",
    "for val in pred_rc:\n",
    "    if np.argmax(val) == 0:\n",
    "#         str_label ='Happy'\n",
    "        final_pred_rc.append(0)\n",
    "    elif np.argmax(val) == 1:\n",
    "#         str_label ='sad'\n",
    "        final_pred_rc.append(1)\n",
    "    elif np.argmax(val) == 2:\n",
    "#         str_label ='Angry'\n",
    "        final_pred_rc.append(2)\n",
    "    elif np.argmax(val) == 3:\n",
    "#         str_label ='normal'\n",
    "        final_pred_rc.append(3)\n",
    "# RC ACCURACY CHECK\n",
    "\n",
    "score=metrics.accuracy_score(Label_OF_test_data, final_pred_rc)\n",
    "print(Label_OF_test_data)\n",
    "print(final_pred_rc)\n",
    "print('Accuracy RANDOM CLASSIFIER: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Accuracy SVM: 0.23809523809523808\n"
     ]
    }
   ],
   "source": [
    "#  implemrnting SVM \n",
    "pred_svm=[]\n",
    "pred_svm=s_v_m.predict(test_Xrc)\n",
    "# print(pred_rc)\n",
    "#  accuracy scheck svm\n",
    "\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_svm)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_svm)\n",
    "print('Accuracy SVM: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 2 2 2 0 0 2 0 0 2 2 2 2 2 0 2 0 0 1 0 2 2 1 2 2 2 2 0 2 2 2 2 2 0 1\n",
      " 2 2 0 2 0 0 2 2 0 2 2 0 0 2 2 2 0 0 0 2 0 0 1 2 2 2 0 2 2 2 2 2 2 2 2 0 0\n",
      " 2 2 2 2 2 0 0 2 2 2 2 2 0 0 2 2 0 0 2 0 0 2 2 0 2 2 2 1 2 2 2]\n",
      "Accuracy LogisticRegression : 0.41904761904761906\n"
     ]
    }
   ],
   "source": [
    "# implementing Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults# all pa \n",
    "# default solver is incredibly slow thats why we change it\n",
    "# solver = 'lbfgs'\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(RandomX, svm_y)\n",
    "pred_logReg=logisticRegr.predict(test_Xrc)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_logReg)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_logReg)\n",
    "print('Accuracy LogisticRegression : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 2 0 2 0 0 0 0 0 2 2 2 2 2 2 2 2 0 2 2 0 2 2 2 2 2 2 1 2 2 2 2 2 0 2\n",
      " 2 2 0 0 0 0 2 2 2 0 0 2 0 2 2 1 0 0 0 2 0 2 1 2 2 2 0 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 0 2 2 2 2 2 0 0 2 2 2 0 2 2 0 2 2 2 2 2 2 2 2 2 2]\n",
      "Accuracy KNN : 0.4\n"
     ]
    }
   ],
   "source": [
    "# KNN IMPLEMENTATION\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(RandomX, svm_y)\n",
    "pred_knn=neigh.predict(test_Xrc)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_knn)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_knn)\n",
    "print('Accuracy KNN : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 2 2 2 0 0 2 0 0 2 2 0 2 2 0 0 0 2 0 0 2 0 2 2 2 2 2 2 2 0 2 2 2 0 2\n",
      " 2 2 0 2 0 2 2 2 2 2 2 2 0 2 1 2 0 0 0 2 0 0 1 2 2 2 2 2 2 2 2 2 2 2 1 2 0\n",
      " 2 2 2 2 2 0 0 2 2 2 2 2 0 2 2 2 0 0 2 0 0 2 2 2 2 2 2 1 2 2 2]\n",
      "Accuracy LinearDiscriminantAnalysis : 0.37142857142857144\n"
     ]
    }
   ],
   "source": [
    "# LinearDiscriminantAnalysis IMPLEMENTATION\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(RandomX, svm_y)\n",
    "pred_LinearDiscriminantAnalysis=clf.predict(test_Xrc)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_LinearDiscriminantAnalysis)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_LinearDiscriminantAnalysis)\n",
    "print('Accuracy LinearDiscriminantAnalysis : {}'.format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 2 2 0 2 0 0 2 0 0 1 2 2 0 1 2 1 2 1 1 0 2 2 2 1 1 2 2 2 2 0 2 1 2 0 1\n",
      " 1 1 2 2 0 0 2 2 2 2 2 2 2 2 1 1 0 2 0 2 0 0 1 2 2 2 0 2 2 2 1 2 2 2 1 1 0\n",
      " 2 2 2 2 2 1 0 2 1 2 2 0 2 0 0 2 2 0 2 0 0 2 2 2 2 2 1 1 2 2 1]\n",
      "Accuracy GaussianNB : 0.41904761904761906\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB IMPLEMENTATION\n",
    "clf = GaussianNB()\n",
    "clf.fit(RandomX, svm_y)\n",
    "pred_GaussianNB=clf.predict(test_Xrc)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_GaussianNB)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_GaussianNB)\n",
    "print('Accuracy GaussianNB : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 2 1 0 1 2 2 2 2 0 1 0 2 2 1 2 2 2 0 2 2 2 0 1 1 2 2 2 0 2 0 0 2 2 0 2\n",
      " 0 2 0 2 0 0 2 2 2 2 0 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 0 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 0 2 0 2 0 2 2 2 0 2 0 2 2 2 0 1 2 2 2]\n",
      "Accuracy DecisionTreeClassifier : 0.3523809523809524\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier \n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(RandomX, svm_y)\n",
    "pred_DecisionTreeClassifier=clf.predict(test_Xrc)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_DecisionTreeClassifier)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_DecisionTreeClassifier)\n",
    "print('Accuracy DecisionTreeClassifier : {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part of code is implementing PCA and ml algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********************** PCA + lOGISTIC REGRESSION *****************\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413, 2500)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413, 2500)\n",
      "413\n",
      "(105, 2500)\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "# Pca_X = []\n",
    "# Pca_Y = []\n",
    "Pca_train_X = []\n",
    "Pca_test__X=[]\n",
    "Pca_train_X=RandomX\n",
    "Pca_test_X=test_Xrc\n",
    "print(Pca_train_X.shape)\n",
    "print(len(Pca_train_X))\n",
    "print(Pca_test_X.shape)\n",
    "print(len(Pca_test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(Pca_train_X)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "Pca_train_X = scaler.transform(Pca_train_X)\n",
    "Pca_test_X = scaler.transform(Pca_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(.95)\n",
    "pca.fit(Pca_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pca_train_X = pca.transform(Pca_train_X)\n",
    "Pca_test_X = pca.transform(Pca_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults# all pa \n",
    "# default solver is incredibly slow thats why we change it\n",
    "# solver = 'lbfgs'\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(Pca_train_X, svm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logReg=logisticRegr.predict(Pca_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 2 0 2 0 0 2 0 0 2 2 2 2 2 0 2 0 2 0 0 2 2 2 2 2 2 2 0 2 2 2 1 2 0 1\n",
      " 2 2 0 0 0 0 2 2 0 2 0 0 0 2 1 2 0 0 0 2 0 2 1 2 2 2 0 2 2 2 2 2 2 2 2 0 0\n",
      " 1 2 2 2 2 0 0 2 2 2 2 2 0 0 2 2 0 0 2 0 0 0 2 2 2 2 2 1 2 2 0]\n",
      "Accuracy PCA + LogisticRegression : 0.4\n"
     ]
    }
   ],
   "source": [
    "score=metrics.accuracy_score(Label_OF_test_data, pred_logReg)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_logReg)\n",
    "print('Accuracy PCA + LogisticRegression : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 0 0 2 0 0 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 1 2 2 2 2 0 2 2 2 2 2 0 2\n",
      " 2 2 0 0 0 0 2 2 2 2 0 0 0 2 1 0 0 0 0 2 2 2 1 2 2 2 0 2 2 2 2 2 2 2 2 0 2\n",
      " 2 2 2 2 2 2 0 2 0 2 2 2 0 0 2 2 0 0 2 0 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Accuracy PCA + SVM: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# **************************** PCA + SVM *****************\n",
    "s_v_m.fit(Pca_train_X,svm_y)\n",
    "pred_svm_pca=s_v_m.predict(Pca_test_X)\n",
    "# print(pred_rc)\n",
    "#  accuracy scheck svm\n",
    "\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_svm_pca)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_svm_pca)\n",
    "print('Accuracy PCA + SVM: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 0 2\n",
      " 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 2 2]\n",
      "Accuracy PCA + RANDOM CLASSIFIER: 0.29523809523809524\n"
     ]
    }
   ],
   "source": [
    "# PCA + RC\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(Pca_train_X,svm_y)\n",
    "pred_rc=clf.predict(Pca_test_X)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_rc)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_rc)\n",
    "print('Accuracy PCA + RANDOM CLASSIFIER: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 1 2 2 0 0 2 0 0 2 2 2 2 2 2 2 2 0 2 2 0 2 2 2 2 2 2 1 2 2 2 2 2 0 2\n",
      " 2 2 0 2 0 0 0 2 2 0 0 2 0 2 2 2 0 0 0 2 0 2 1 2 2 2 0 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 0 2 0 2 2 2 0 2 2 2 0 0 2 2 1 2 2 2 2 2 2 2 0 2 2]\n",
      "Accuracy PCA + KNN : 0.4\n"
     ]
    }
   ],
   "source": [
    "# PCA + KNN IMPLEMENTATION\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(Pca_train_X, svm_y)\n",
    "pred_knn=neigh.predict(Pca_test_X)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_knn)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_knn)\n",
    "print('Accuracy PCA + KNN : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 2 0 2 0 0 2 0 0 2 2 2 2 1 2 2 2 0 0 0 2 2 2 2 2 2 2 0 2 2 2 2 2 0 2\n",
      " 2 2 0 2 0 0 2 2 0 2 2 0 0 2 1 2 0 0 0 2 0 2 1 2 2 2 0 2 2 2 2 2 2 2 2 0 0\n",
      " 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 2 0 0 2 0 0 0 2 2 2 2 2 1 2 2 2]\n",
      "Accuracy PCA + LinearDiscriminantAnalysis : 0.4095238095238095\n"
     ]
    }
   ],
   "source": [
    "# PCA + LinearDiscriminantAnalysis IMPLEMENTATION\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(Pca_train_X, svm_y)\n",
    "pred_LinearDiscriminantAnalysis=clf.predict(Pca_test_X)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_LinearDiscriminantAnalysis)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_LinearDiscriminantAnalysis)\n",
    "print('Accuracy PCA + LinearDiscriminantAnalysis : {}'.format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[2 2 0 2 2 2 0 0 2 0 2 2 2 2 2 1 2 2 2 0 0 0 2 0 2 2 2 2 2 0 2 2 2 2 2 0 2\n",
      " 2 2 0 0 2 0 2 2 0 2 0 0 0 2 1 2 0 2 0 2 0 2 1 2 2 0 2 2 2 2 2 2 2 2 2 0 0\n",
      " 2 2 2 2 2 0 0 2 2 2 2 2 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 2 0]\n",
      "Accuracy PCA + GaussianNB : 0.3523809523809524\n"
     ]
    }
   ],
   "source": [
    "# PCA + GaussianNB IMPLEMENTATION\n",
    "clf = GaussianNB()\n",
    "clf.fit(Pca_train_X, svm_y)\n",
    "pred_GaussianNB=clf.predict(Pca_test_X)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_GaussianNB)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_GaussianNB)\n",
    "print('Accuracy PCA + GaussianNB : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 3, 1, 2, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 3, 1, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 1, 3]\n",
      "[1 2 0 0 2 2 0 2 0 2 0 0 2 2 2 0 2 2 2 0 2 0 2 0 2 2 2 2 2 0 2 0 2 2 2 0 2\n",
      " 2 2 0 2 0 2 2 2 0 2 2 2 0 2 2 0 2 0 0 2 2 0 2 0 2 2 0 2 2 2 2 2 2 2 0 0 0\n",
      " 2 2 2 2 2 0 0 2 2 1 2 2 0 0 2 2 0 0 2 0 0 2 2 2 2 0 2 2 1 2 2]\n",
      "Accuracy DecisionTreeClassifier : 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# PCA + DecisionTreeClassifier \n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(Pca_train_X, svm_y)\n",
    "pred_DecisionTreeClassifier=clf.predict(Pca_test_X)\n",
    "score=metrics.accuracy_score(Label_OF_test_data, pred_DecisionTreeClassifier)\n",
    "print(Label_OF_test_data)\n",
    "print(pred_DecisionTreeClassifier)\n",
    "print('Accuracy DecisionTreeClassifier : {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for LBP and Ml algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "# \n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 413/413 [00:04<00:00, 89.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the training data\n",
    "hist_train=[]\n",
    "label_b4 = []\n",
    "hist_train_label=[]\n",
    "for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "\n",
    "    # labeling the images\n",
    "    label = label_img(img)\n",
    "    label_b4.append(label)\n",
    "    path = os.path.join(TRAIN_DIR, img)\n",
    "\n",
    "    # loading the image from the path and then converting them into\n",
    "    # greyscale for easier covnet prob\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # resizing the image for processing them in the covnet\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    lbp = feature.local_binary_pattern(img, 24,8, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(),\n",
    "        bins=np.arange(0, 24+ 3),\n",
    "        range=(0, 24+2))\n",
    "\n",
    "    # normalize the histogram\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "    hist_train.append(hist)\n",
    "        # final step-forming the training data list with numpy array of the images\n",
    "#         training_data.append([np.array(img), np.array(label)])\n",
    " \n",
    "#     # shuffling of the training data to preserve the random state of our data\n",
    "#     shuffle(training_data)\n",
    " \n",
    "#     # saving our trained data for further uses if required\n",
    "#     np.save('train_data.npy', training_data)\n",
    "hist_train[0:5]\n",
    "for val in label_b4:\n",
    "    if np.argmax(val) == 0:\n",
    "#         str_label ='Happy'\n",
    "        hist_train_label.append(0)\n",
    "    elif np.argmax(val) == 1:\n",
    "#         str_label ='sad'\n",
    "        hist_train_label.append(1)\n",
    "    elif np.argmax(val) == 2:\n",
    "#         str_label ='Angry'\n",
    "        hist_train_label.append(2)\n",
    "    elif np.argmax(val) == 3:\n",
    "#         str_label ='normal'\n",
    "        hist_train_label.append(3)\n",
    "hist_train_label\n",
    "# svm_y[0:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=100.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearSVC(C=100.0, random_state=42)\n",
    "model.fit(hist_train, hist_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 105/105 [00:01<00:00, 87.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.0704, 0.0412, 0.0384, 0.0308, 0.0176, 0.0124, 0.0056, 0.0052,\n",
      "       0.0048, 0.0068, 0.012 , 0.0204, 0.0188, 0.0224, 0.012 , 0.0132,\n",
      "       0.0032, 0.0072, 0.0076, 0.0076, 0.0056, 0.006 , 0.0072, 0.012 ,\n",
      "       0.0172, 0.5944]), array([0.0524, 0.0528, 0.054 , 0.0312, 0.0144, 0.016 , 0.0096, 0.0032,\n",
      "       0.0056, 0.008 , 0.01  , 0.006 , 0.0088, 0.0156, 0.0088, 0.0048,\n",
      "       0.0088, 0.0084, 0.0124, 0.0112, 0.0076, 0.008 , 0.006 , 0.0156,\n",
      "       0.0256, 0.5952]), array([0.062 , 0.0468, 0.0224, 0.0224, 0.0204, 0.012 , 0.0084, 0.008 ,\n",
      "       0.0124, 0.0196, 0.0256, 0.018 , 0.016 , 0.018 , 0.0116, 0.0084,\n",
      "       0.004 , 0.01  , 0.0072, 0.008 , 0.004 , 0.006 , 0.0096, 0.0136,\n",
      "       0.0136, 0.592 ]), array([0.05  , 0.0492, 0.034 , 0.0424, 0.0176, 0.0148, 0.0068, 0.0072,\n",
      "       0.0128, 0.0184, 0.0192, 0.0068, 0.0088, 0.0132, 0.0112, 0.014 ,\n",
      "       0.0112, 0.0064, 0.008 , 0.006 , 0.0064, 0.006 , 0.0088, 0.016 ,\n",
      "       0.02  , 0.5848]), array([0.0876, 0.0432, 0.0544, 0.0284, 0.0232, 0.016 , 0.006 , 0.0048,\n",
      "       0.0052, 0.0148, 0.014 , 0.0096, 0.012 , 0.0124, 0.0064, 0.0092,\n",
      "       0.0084, 0.0124, 0.0068, 0.0132, 0.0084, 0.0068, 0.0056, 0.0192,\n",
      "       0.0212, 0.5508])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading testing data \n",
    "hist_test=[]\n",
    "label_b4 = []\n",
    "hist_test_label=[]\n",
    "for img in tqdm(os.listdir(TEST_DIR)):\n",
    "\n",
    "    # labeling the images\n",
    "    label = label_img(img)\n",
    "    label_b4.append(label)\n",
    "    path = os.path.join(TEST_DIR, img)\n",
    "\n",
    "    # loading the image from the path and then converting them into\n",
    "    # greyscale for easier covnet prob\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # resizing the image for processing them in the covnet\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    lbp = feature.local_binary_pattern(img, 24,8, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(),\n",
    "        bins=np.arange(0, 24+ 3),\n",
    "        range=(0, 24+2))\n",
    "\n",
    "    # normalize the histogram\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "    hist_test.append(hist)\n",
    "        # final step-forming the training data list with numpy array of the images\n",
    "#         training_data.append([np.array(img), np.array(label)])\n",
    " \n",
    "#     # shuffling of the training data to preserve the random state of our data\n",
    "#     shuffle(training_data)\n",
    " \n",
    "#     # saving our trained data for further uses if required\n",
    "#     np.save('train_data.npy', training_data)\n",
    "print(hist_test[0:5])\n",
    "for val in label_b4:\n",
    "    if np.argmax(val) == 0:\n",
    "#         str_label ='Happy'\n",
    "        hist_test_label.append(0)\n",
    "    elif np.argmax(val) == 1:\n",
    "#         str_label ='sad'\n",
    "        hist_test_label.append(1)\n",
    "    elif np.argmax(val) == 2:\n",
    "#         str_label ='Angry'\n",
    "        hist_test_label.append(2)\n",
    "    elif np.argmax(val) == 3:\n",
    "#         str_label ='Angry'\n",
    "        hist_test_label.append(3)\n",
    "hist_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 0 2 2 2 0 2 0\n",
      " 2 0 2 2 2 0 0 2 2 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2]\n",
      "Accuracy LinearSVC: 0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(hist_test)\n",
    "\n",
    "score=metrics.accuracy_score(hist_test_label, prediction)\n",
    "print(hist_test_label)\n",
    "print(prediction)\n",
    "print('Accuracy LinearSVC: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 0 0 2 2 0 2 0\n",
      " 2 0 2 0 2 0 0 2 2 0 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 0 2 2 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2]\n",
      "Accuracy LBP + RANDOM CLASSIFIER: 0.6285714285714286\n"
     ]
    }
   ],
   "source": [
    "# LBP + RC\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(hist_train,hist_train_label)\n",
    "pred_rc=clf.predict(hist_test)\n",
    "score=metrics.accuracy_score(hist_test_label, pred_rc)\n",
    "print(hist_test_label)\n",
    "print(pred_rc)\n",
    "print('Accuracy LBP + RANDOM CLASSIFIER: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 2 2 2 1 1 2 0 2\n",
      " 2 1 0 0 2 0 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 0 0 2 2 2 0 0 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 0 2 2 2 0 2 1 1 0 2 2 0 0 2 2 2 0 2 2 2 2 2 2 2 2 2 0]\n",
      "Accuracy LBP + KNN : 0.580952380952381\n"
     ]
    }
   ],
   "source": [
    "# LBP + KNN IMPLEMENTATION\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(hist_train, hist_train_label)\n",
    "pred_knn=neigh.predict(hist_test)\n",
    "score=metrics.accuracy_score(hist_test_label, pred_knn)\n",
    "print(hist_test_label)\n",
    "print(pred_knn)\n",
    "print('Accuracy LBP + KNN : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 0 2 2 2 0 2 0\n",
      " 2 0 2 2 2 0 0 2 2 0 0 2 0 2 0 2 2 2 1 2 2 0 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 2 0 2 2 0 2 2 2 2 2 2 2]\n",
      "Accuracy LBP + LinearDiscriminantAnalysis : 0.5523809523809524\n"
     ]
    }
   ],
   "source": [
    "# LBP + LinearDiscriminantAnalysis IMPLEMENTATION\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(hist_train, hist_train_label)\n",
    "pred_LinearDiscriminantAnalysis=clf.predict(hist_test)\n",
    "score=metrics.accuracy_score(hist_test_label, pred_LinearDiscriminantAnalysis)\n",
    "print(hist_test_label)\n",
    "print(pred_LinearDiscriminantAnalysis)\n",
    "print('Accuracy LBP + LinearDiscriminantAnalysis : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 0 2 2 0 0 2 0 0 0 2\n",
      " 2 0 0 2 0 0 0 0 2 0 0 2 2 2 0 0 2 2 1 0 2 2 2 2 2 0 2 0 0 0 0 2 2 2 2 0 0\n",
      " 0 2 2 0 2 0 2 1 2 0 2 2 2 2 2 2 2 2 2 0 0 0 2 2 0 2 2 0 2 2 2]\n",
      "Accuracy LBP + GaussianNB : 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# LBP + GaussianNB IMPLEMENTATION\n",
    "clf = GaussianNB()\n",
    "clf.fit(hist_train, hist_train_label)\n",
    "pred_GaussianNB=clf.predict(hist_test)\n",
    "score=metrics.accuracy_score(hist_test_label, pred_GaussianNB)\n",
    "print(hist_test_label)\n",
    "print(pred_GaussianNB)\n",
    "print('Accuracy LBP + GaussianNB : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2 2 2 2 0 2 2 2 0 2 2 2 1 2 2 2 2 2 2 1 2 1 2 0 2 2 0 2 2 0 0 0 0 2 0 2 0\n",
      " 2 2 2 0 2 0 0 2 0 2 0 2 2 2 2 0 2 0 2 2 2 0 2 2 0 2 2 2 2 2 1 0 2 2 0 2 2\n",
      " 0 0 0 2 2 2 0 2 2 2 2 0 2 0 2 2 0 2 2 0 0 2 2 2 2 1 2 1 2 2 1]\n",
      "Accuracy DecisionTreeClassifier : 0.5238095238095238\n"
     ]
    }
   ],
   "source": [
    "# LBP + DecisionTreeClassifier \n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(hist_train, hist_train_label)\n",
    "pred_DecisionTreeClassifier=clf.predict(hist_test)\n",
    "score=metrics.accuracy_score(hist_test_label, pred_DecisionTreeClassifier)\n",
    "print(hist_test_label)\n",
    "print(pred_DecisionTreeClassifier)\n",
    "print('Accuracy DecisionTreeClassifier : {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
